{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a18187eb-f7b4-4991-88bd-4070c97ae6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.0+cu118\n",
      "torchvision version: 0.20.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f0069be-0c5d-4a04-9133-4567b95e7cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62f0bd0f-caad-4860-9ae4-0eb1621e06a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random image path: ..\\data\\b200c-lego-classification-dataset\\2420\\1664.jpg\n",
      "Image class: 2420\n",
      "Image height: 64\n",
      "Image width: 64\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAeqElEQVR4AT2a2Zbc9nHGse9o9N6zkhQpiZZiJ3Yu8ki5zGXeKSe3eQznnOT42IqohSI5nOmZnl6ABho7kF+hZcH0CI21qv5fVX1VBfU//+O/TNPs+z7P87ar2WdTFKVta/5WVVkUBTuGYbRty37T1bqmaYra1U2dF0VdtZ3S6mqva6XS1nXNZVqn2LrhGpah62rX9l3naM7IC33NMRTdVC3bsvpOXazmThSmdZbkWaU0tdLwNM93TNs2NK0sy7qsdF13bceyrKaqkJD3+r6vqurpdOqb1h6u1JCet7I1bcN+08hfVZWDRYGMBTegFQdruaCTs13flDyx5EF5WVRdq1qGaqKmwcVK05enrKyPatf1msrFtmH1fm2NFtE4XM5Xk2isqUYwClVH32dx3lTH+FgrnW7rXdfxXmX4iyQiVt2IALpeVRUKYD3egk3ZuEDsitnYON0rLcJxgoOGoXEQM3Cc+weVkKfLypKzXVW3/Ctq1kHpFEPRuqrrqvLUZGVetEVjKn1guI5rjecz1/WXk9nF/GIZzUfeaOyHgRsoOjf1rd7zCEvV89Op09SRH2m6pmOCYUNW3shKI6jvuhzrlL5qao5j0E7XRU5BDrYfrK7pch/3sGHdQYGCUyjANZwCDQgoZq3bvqxRg1Wuy6ZhBXRMjbFVpzUdy5mE0e3yYjqdzq4uwmi0nCymo8jWHKXulaZVehVZWLlG7y3VDGzPM91a7T3bA39ogL0wM/bmpT3IrmuOnPVBnlY3zjjHuAa/2ZCYtyuKCDrI36VpymkWRnQatOIyFKhUDKfpLFXVNXldZsBIVB1NxuNxdLm8XM0Wi9F4PhovokkQhqpnKyw/jwfjdaFjZ0WAgA48hDeajjsfz9JTtsuOTYUDNVoncoMkREJQrnYc53g8ogOw5k+hllhAxRRACC9BYm4Q83UCJ47ykx0UaHAtbAD6zz/lUk1pO61q1bp3VH01Xk1fjsMwxNZRFIHvGTC3XJwYrCo6b+9FTnynrEV6kKPoPJHVNDQLl1E0PbD9URDFaba5Xxu+oTsiFS/lL1LZpnXGDMYVfA+iIqSuCpDE57iOK7ihqiuJM03DT65DWuRmhzvPzsB7bdMJ7GAS+ZEbTvzwYra8vryaz+eW43IXrolhBGDEprLFNTpD0y1TMXTsqGBa8BMfyzixXEeNQh6n1OBIj7zRRt9mx8zodKuzMbkYFDU1EKwhIYhCHtUQ3UCYKGDqrufJUUQ/q8U+J9i4GqGRHn3OByWo1TV+dzmav7l6+fuvvn11dRPantZjTd6l4sw9WmMyDYkNkdXCBqoGkoFy1SlVqZR1n2S7p83z0+bq5jp0HMVDK4m5k3A8w71tL2ty7jvbFKmQ7bwhD4JxhFOIhDCYSxQjxCIlv0WKYTXYBzz4AFegSRzHxEruZEPxVTD909e//8Pbb9S2r44nw7QUyyZSuBZYF8zgQ6IRAQGzsH4166AJlkxVKfs6r/bP283DGlcIx5ES+iw0VnVNYzGZ/+6rr+/293YomMFk3HheCvbP4iI6GzZCbnB/zDIkFBRx9Cwu+pxtP1zZocl5Q3ouMBvl6f7h+e4hv7jxgpFlOWd35HmK7yiSABWA3jYK8iktztIaro6N5V9LjFMBLmACA1WWZ/udD4rw0rrRdNNzXM9yyHFnAbA3N/GXn6LMYHvEQ1rWhINnkQQqXMQmCphictblvCC/neIs93Cn0qrPT4/r+7vt5qXn+Iph8gaRGjBIDOMlIqskaoDTg3wdn1e45BzMeLRJkLVd2y5O6e75yV/MlGnU16Vqqp5lchyXOeUkQcnBbIh+GtwSO7quS7DWO4E92UDU6DtZKX6fdUJuNuTmCFKdjwscBhdHjeF083zYb7bPRFAxMmEKg1sS+IaoLX9V1Dj/I+QgvQBJHidAcqwwDMajkIAX73fNKRUNoRuteLshuaQH0oCWt5+tiRoYnu0MFkQVLAybGJ33co6LBCp1gT78F1nZOSvDbVzAZfzstT7ryvVx+2m3vi1eLfywx2v7VqlqJSTUiNUFOfUQJ9Gu6i1N1VpwxVHyV4+fwHZGUbDbP2WnNIn302quSsoieIkkmNlIDbXgFyFH4I2UbMjEPpsI7NicYgdbC2BQBrXYgeAg5fkiDMBpjp8vZYcber2L09Pd/nH1eP/lcTubzTTbaKrGqAqlMgXoPBRuMexA6cS43CeIktSl9FCBjrjkukDdPGTHzePjZDVXp2PCF/cGnjebTI5VhgX5eQbCWYHzT4QSUU3JxOyIAmeJuQgF0J4TkAtWkKzETllKGGVNzrgin1Z6vznFP60/vrr7OJ8tZ+GEhATstFOuwTExd0s45XkiPWhqEBsFAI9E3AYWi9c6njMajY5Fut1tnp7Wq3GgdFpXNrZrRVo0yYkumQApzXgQSiM9omBKzRD+cw6brAkSGsiKunJO02CnsoLD6nieZO8sS8ESunIPTyG4uL6XJsenzebu4dP25ouR5buGTTYVcjaYUYeqghfAD4SIDQCbuCP34tC1/LVs3W1RYM92PKSHeFVyXK361nA8T3dc0weOp5K1bDRilKZCdXiAybJho8HwZVUgPXpqQRBMJpMF/GU0QjmE5q0cxOqoxA385CD3ox7KYBfXcbI0/fH/3h2enh3M22pK3ipZoxQdKMLWwAEdAIEB66o7gz0EEDCpkpJJHYZNOp8GUaDah8+bzfuPXOAEAfQ21IOFO7sIlqEVmqpJagcaoE6zsAyRkMoE0m2Yquap1sQJDaAi6B9wgtDn1UFLZGUj8OASXIAaHGQHBt1WuVrUh8Ph7tOHzeXL5fRCUAJP4R+yDg6JT+q4JX8RvWI9elkFKDG+hCZZYQXByA92mgEdTHaHGWghiOpmW6me7o7cKLZiYiVryYsJl/AKHqO3ui1WJSSZPeG0aSVu4QbIjfSYGRHP2/kga8LFpEMOciUeggORWpq+S9LjD+9/vnv8DOFRXFPAjY8ibiu7Vq+QKg08AD5bF0pbyT/YhNDbVoFreI4TQh3coqqTw+G4OyhFiZptU1GCjaMIFPDScyw5eyBGxKbIyUHQAdQRxoAy8BvrsqEAG+fYuAcdWD0R+u8PwhZEzaaHxPZZlb+///DL/ce3b7/xhRFgfAIlmgwJoVNU+A+RN0skShpUp1VelRAx33JUFK4rw7Ft31PjPVBOdvvRbKICDYmS+sj3PFfYYVtK0SPk2TCRqGyknGrrFplUXATvRUqERkT+8vs3nxZVhg3FzmbgSoqjkvqI8kUF9s3j/vmnu/d3T/evPRdrkKglY6ED6wAPyAolP9V5Ujd51lQ1QNA1TG4TBHUYlK64jhV4hmWe0ny/2S0XK3s0hl+z1JZp+g5XWkQqat/ekGiOmZuWKopCpAOnRAkkGkrYQQFuM2qJRWxcfa6dERTpUQ8goQ4VPacM29AJGx01ePlh/fndp5+nq8U4nLLeGihCjRb9CiVOTsm+KA5Jdsjq0g7DycUijCJiuDh14BHJvcnI24Xp6XTYbLPtwb68VC2N7M57A88fB+E+PWAxkhl1AxKKDNiT9I8jNcIYRGL0YA+osM9f0cQwiKfIDdqJVhwhBHG2akobwqQ7bUYdr+Fjd5v1u19+/vLrt0EYSZJisUEi1fXpVBx2x902PqwbtbGjYDGZRLO54uPHkqIIpnj2pF1VJXwiy+LksH6aXl4ql7O2qCh1kH46nqx3z0lxwiZ0JcCOMDtIHiCQ6hBjQcdM8+zBCE3xed5HVptSgzf8PRyJiwxlBKwLxXgY5SYskoTw6f7j9rC9vb4hFuldIzrgu1VZZ1mZHJ/X69nF9Gqx8m5uFN+T+IqPu7askmNpi+mq7/ePj8nzLtnu86dndzWjCtYN1fM8dHCwVzuQFSi/lDFgsYTxYHToE4uiJUnCUX4TFnEm9MHY/MS5Cf9smJ+AgHrIDXeg7nYMZ6iiO6ITS3e/Xv/y4cMhiW3XQUnhpybB3gDpSXLA6ckz3nQmBJVAh7M53pDUgLGq2I62mC8vVjwqjeP13R1hSA8ICUqdpBT7y+mcJw0eRIiTeotWwDkW0dbJ8hTBZBEQDkGbViIUQvCTeMXV7LMa57NoBYTSPDUhM7ZDh8pstLRN1vf3//3nP7+6eTmyg9DwRCzSgoGdKSpLx/fEf8jucG9OEcxZIhg7+KtrB1V1w4/G9C+aJIXcKM87xZiRDk2qSEufhmPPtPeHAwS70/EwsiaPoSSQaMRGyflrSwJBkZ6fHOUKnBiroBsaIsFvSlKR0Duh83OKs5J8pBJ99Cw53t99Bn7AS9gc3M7UnXEYTKa0UdJTpeQVgVDRbDJdWwBfTAw0kJ4Mb7rT8Wwx53XFKTs8P3OD+GhLVarNx5N5NMV5i1OZZ7CjHIeRfxkFEYdOBrAZFBrwNTgJCgzr0HKKzhxncQaiOCtAECuygvAtrKSWvgjUId3G3yV/e3Pz5g9f/z5yRpiBaKAEVGLzWXK5O+4TCfMH+ixKaCs64RPXc2xQhr2JOTyfDtjIzza6JITtfryYKx7eQl2nhbZ7vVg9UEbtn08tbB/4y1+Yf98Kwf61eEdoHqvSKhvqN5G1rYk/9EZZGTZW5rw4dCVgmPWpKuOU5gpmdFjpXsVjTseU5E+LStGhd9hbd2fTq1evqqJMs9LfxboKSTPwXSk+oXYogTbCmTSTnOCTlfN4t82TJTcKO2xpNtqzaHo5Xz7HO7WE7wMhaWESaen/ACTJAwAduZGSoIOUwoHKkigE9MEPF3AWZXBojtOBafKqzStLM+az+evbl1++ejOdzKeLxZs3X0FnawJcW0IuNdP2L6l5wnj9dIyz/T72iS6eq488TXMVT3SQtI2vgNgorCZjrLZ53pET3NWlMg7lXA/nEIjxUiCEJxi2aulGKRFByi+h09gbzCAuCvITWZEY1jnApkI9znINmkh679VxtPzy+sXrmxdX0eJivljNVsQfLwiV0aiJ01ZlaXGEGjagzCPFC6OebBsPvoaBOignrmKZltTNLDy0G2NBhsdhdQLP+3i3X253OhkDDeuGzDMdjwPXS/KUjjI1ibSzBlJIXWoQPRFRYiQKoFGWwfoAsXC/oqZUoYZwXTh3MJ5O6J/Rxgq98M31i+v5Eq6mNWRAkrRaZaXVHeu8dMcTxdH7RJoiUmRCr6cTOlBKTQiRtqI0ZeGmGF5IDFam+HeUoFf9sDV2JNAqL5L9bjIbSzjubduzJsHoenUNlTpmCdGro1OJuJR1UAzSGm0giJEB2a6aLsv7kiikhZp54S/nl9PpYj5bLGdzGphzj9g0CnFgGDkklrAmHk5mbTuLJgUFrR1KfVw0KlUOp+iGQIpg1QwGJDGyEZUhBhIYhpTMBdJW6YouNwJ3cWntdmpXxYeDtz/Yt9fS72haR7O+ffN2yFRprbW7bJeUxamvj2Vm0JolqUrTuqHyoN1pjqezSTh6fftFFI6Wy4v5cgFNMB2b2gpz7pNYalutM6nn8VScEmGAAVGyqOi9QTOpAlSrlcCPwUe6uGmNlHR3xctUsoTcQ/du0IGfmoHCqu3rTkHeOG6O5X4XpSubnMi/XmXmQWt+NZ5vtk9plxMFuzKhYlMdyyh3KTF+5AST+Wgxnt5eXn/1xZvbq2tdZcCCxJZggPfiX11D/4ECSOwntcvQZcDAVJtQt1Mex8f9MSVMuEEUzSY+2dcf4szgpjWNBxUMQcqklcEjhyb7UEXAl3WVQYxZ2/p4kjytKTZH8W5yuqRRITMdQwovOrDOJzcm31G1ygylpA1sfPPizdXV5e++evvy9gWUQVIc8allBCDcVWNvqBCkdKArRp6C7+J5Mleifdsgd7HbUo48rtfch9WD6dQYCUBERHQ/SWeaqCC/pTchfRJWAHcm/v36HPTBNQ2lFxrtU5mt493Tw3oEdKFP2BFC6lgXyxU9i4fdmqLOxLwadFY3/v1f/62V4ElPoK7SQgQ1LcMxJK0aEiuE/QpOZCQhTDmnk6XSu1MqiuC6TdJ0s9s/bg5PG7zl6vbF+EzalBau0h1qms3YHLml7zdoJFEN3cjnpEH+yXM5r6tkE0of2yBaOMl+u914958Bsel6dX4irqxm85ura0qoIq8JJGXXpFVu9FmBiAZlowvHGjgMUb+pwqvZWWJeJxSZUCpVL5ga6hWWoisU5kO05LM8T7PleHp1dR1c3SijUAyvCLFvNVPCHgtnIKA0r4g5rL4wNn4PPwcFuHxwa15gGdP5bJIc4sc1fWX4g6lM4EWkJ8u3rxaXZJzdXSLVERVy0RpahzPpMPg+L6S757ua7+BN1HIy6qDGHkYMFDHSxUOyoXJUSlrNeCdFHsmaQq+aXayCyVhxLaXIZY5DrUjzXtObk2ggTS4kpM4c4jdLIV7EvkCRJwJLKmZWlUknsVmlg8JdMOWnpydvMtGCUJV6umF6cnt59cvnD9khLZnnFI1xuLuPJmNmDRSdmFU2lnSwjunwP1Oh8ODdYDgnLLL0wzk6KCfhUieWE2ougjD8sBTPpbFIUKZ71VZVVpW26klDb3jGgEICGO+g2B/6j9JapbziNAMEpoZlIb2oI4OrnLWtD5/uP/vT6cp2aBfBjkI/uFldUm3Wp7LpaLJpxue7O9pmkVTl0mrUqWOQ2JW0IEKTazJRSSJQRcegprUM5TOqWviuS4/DrPsup5mMqohSDFcPWBG1OxaaPEwza9CgZqmho0TGTvKDRMmqLQvchWkhpRbOfjjEuF+r68ura38SXb14sVwuFWewouMaigwiJqNo5PlFUqvE6r+9+5vLvOj2Ao7O/bKs0kmW9xE0pdUwtDgJeURBwfKwGmRDGicwLT+kdB3BO552G5d8P59KtdVhVyJmFzDeIiuQgwn23EibpyxoOXVNNUw4aa0dIcZ1weSzO485Z/MFxOMasmBCAWwLz6RQORykD003y7Yu5rNvvvz64enxkB512zX+8uN3qy9uXrZf6sAX6ZH6jHKWGXWkcJbD501aY6IVtSddToZIlh2NCD5ouXvebPe7leeY6ADfFG5ISDAVz5J8XLRKnnXwLOg4/f6qgnQRsMkuEBnTogfhQCLI8d5sIV0jGnj4G5gmC9E7KSsh2ADPwB3m33799q/vvnv3/kdcxfh82Px4/+Hm8fWt9YKrIXAUKNTwGEzAih2lDcCeRBbWBZjBfySccwUvoD01iSxdDTwXNn/38dP4lE1QSagYebBT0kahAjkcqFF5uPS8h1k/OtMlD70R7IQ+rDTtJGcRUYa16hjuNOL30CFQDT5JCGAvyxArcJxx4LtwMDqPcZt/9+mnxfurcDmbTWZ5SghtPXggsV96g9A6PHSQnmDCE2mmgweiO6fhYSwODIhWyygy7u+Yf8WPT+XxCNcSxJnmPjlVfMIA3IeBl+MxgeTxGjUkUKE7T89VwrdEJOkjSlsJyWzmCvRBJcFJM4/j8RGC0qTSe6/UfjwK5uMxNaFRGt3ddv3zw8c/VH9ceNd23VAoCqWRrP/reEvaqhifA2ywVPRgn/cR+JpiaDSoRE86CIhHWbN52hKfsI7hOdSmGv2rIKSTG47GNLcVMgnA4C8POee2s5/BMRCZqppgiMVYdTYWHxRDN2E+FsM/GTRyKHTx8DB7TKgMiOXHz5sH/l1fXMOr6YeJXWVuiqDQX4mh4h0CainUxbX5R5UNb6PvSSikVbrbam1P85ulM2wv0m0HD/a98NW15lH2BpIiMLbAWppU1M28hFUSbwHxcpxkDOcT0XkkeCNRmjBGdOAOLhhGq5QHRFn8xwJdNLaKoRPxuHt+f/fxanG9imY2hd+vbssreAm8Db4tziywIVBSH9OO4sbhMwym4y2MvCgHa6o2HPZqrERjxfdlAukACenxD7ZEc9Z28FHPJ1XJqnKKFcbR4MNlX+WE0GEjkPPVBOtLjCXkw6qyhBUgw8fFMaVYF75Jh8SgGaXsD8n//O9f/vTtP/dMFHSnPhUoIam/zYVsMkUkXZO+aZ7Gh+FDm/xUFhSPmI9mvWYa85sbqhZtFNGblRh8jsgIwPwPEdkwodBb/GewN7ILF8KRBiWkGUy/rYJD9lRtLcLpwIEIUiGBWq3X95QuxImH58eHx8ddcuDU9GIJ5nTGsFl+2m73d58fFuMl3y3ImokTE8IpLtNTcqChURaplOdpKsHOo0AD1JFFJUmPiAACMuGTDIyRC9RiGZ5g6RVOwqACSowajGfELoM+Qshp4cDr5FriI31/XFdhOgNRpteYHfmOhI9ASM3k+83hmf0kO0JwKIq92ZhAzEYP1B46dSXs74cfvv/iWgYWrJRELr52OiaMVIss5oBlOzRcL1++ZOFMy1XI7UjMpRJkOxl0IxmNDaLkGdYwC5tBgS8riTthb2GE1N9YRtILmCeXI7p4c90wM6rq03ET0zuDBR2zNCNpSAsFskUZrRzpBFUFzuA4MCVYWVXnDOikoUtfuaUr8v2P3//D229fX78aICvBC9QjrqdHhHk/whEZxw/RQ94pSJF4AU7kvwCdfYl8ckDsfb6ATDU4kMwWQBE3SdCEnhKCAEvNV19ZESf7+HBE9F8+v69aaQqyNLJghPHBRxzHDhzTU0YyIRq+xKJZ7wQ+vP8EkeoBeFPf3999/PjL5ouvF9EMtenHuYuZOx5LzBFpMCRsmB8ycUBzjAHLF5VAtqzaWWKBCVLLT5q8NRyQgQrjMqHVrJK0lqE9XUv3FdxuNo/bQwynQGbiY0KQEB/R6dEyGgPhPIqtoBYVssHEWRhPR1FHuKRo5LmGadNTQBZaLw9P6/v15+V8oVEaAmKWirfSJ2FDcqDAHHH4PAf7oUcLryAc1Z0zCkTuobUqwYV3D+HchsyQVOkqMxXNBBs7BkpZyhG6DID4mKY0clgKsQgUyJMpFhuFLprwDnDHX0Z79EwKmpaAgkkArY1CxsFGCDC6fn+qABL0+uHx/vuf3715/do3A6G4OBnlHNYk6JJBDabu1HEIJ1020Y4nD7FEfEuAyackw0oRSGB8zFLUDuvSCmBDVrpjSZbilAhFbcSm2prt+CQQuR3eVhUUEDBcCkQ+iEEpjtPIYLQMB9EIU8M4GK3gVFA1g1lUhV4MHg2V+I753/30wx//8Z+ulze+5Rm4C1QWBQQhrFyve6ZkNoQWGjaE2mF1hBdwCcuFd5V8rZMc9uKOz/sn0L3dbqk9OQX/oQtmupbjuRI/uRhVQY+st3SnHN0FXkgmZbQUnDLe5RTfEvJ27paFwjHowDKFcRyikKwZzIRvN5nV7SlG98/Pu+fb25cGgQWXA5lkSh5FA8elSSojQrG9MQx+QBXIwQks5ZSkG7bnp3jPqOEoc/62ZEgWZwlySOwyTTf0EF3mE0pD45L/ZpKStMCXmSRfaQhZBOngR2ZF0ugnH7ARxZlrYiKAVOUlWpu0wkzbgHFbphNGjMdOfFLIWhyy+OdP799++62vBZTjdIxMgKBT2kv8kG8JsTLGo3NXNNkJQXMQfnf3cSCdsM5k6EPybhM/YSrNcxAdHcV3JaTwMEmgbKQZQbn0bKif6XsYhFbxdEGjGA9jU5fykQ3kh+ulDyDpsyHuIA3YMnAyGg9A96hbfIXIDIMPWd+9/+lf4t0oilyGJcRsXkmCoo1AvMZjgVxRJMfDbrfb77fS9K3kAxn5BlMIs3y4QG8Yi5P2Td/1GSY5gmDcQCoASlCJtUMQkCYLtJ0mDklP+iFUZYL6oeVMumCHlYYQ0XXgSm6upC0jHRluAVQGdIvMioQscmPzQYUJ/kiBpAXUJBOprXzqQquUNI6U50kUchNPCOCyoLhOUwJC07OI1XARYgSFOSbEU4ccJLEcc+H6g9SS0Ejq8tXMwLH5SX0qjT4GpZAl5JOxpIBT/j80AkTmXwsJEKBKU2nopRthGMla0pFhGOs36lSKP+a4pypP0hj2Q1scmDKz2x32xzg5psnpmBBJkI85HPHDDMy6Zu4r0EJEJm+4t8xC2AbIQjn5jBFfBFS0x4ATi8AasqQWK2XY4B5LU3NXWgmlk7pjgBEY4jiqoz9IRkzSED9BGfm/50tZwvUonFAgAkkLGtN1wElosKI+PW9YWUSQ9rVgSAPrvLUqM5yo11sqwDDy+XpSkM0nY44NnPFwogp5XbDLyrKgrkerj102JOA4MnELagBu/onoWJSNLoRkaHI0VwmB5VLMxD7rCZp4vkRs1ob+CebqhV8ir6sUlOewEo8czRNpmHJbfDwAJAQmlvBdhhQiQFo+tegsn88geiTo4Wo9BZzMzgNmphQbqExCGNr14B6hiSbMQHksckjkFljIhpnAMW8UWZFKOJ6QIvoGuDtJfgChpDCJthw35VM5YiC9kd/WgRAkAxgYD6UOnkd4l9wjkVJGqLwGDCCEF8jADzRzko9CpdXJO/nysJGPCeGYhHZqggG6kvyl+6b01KbEFpevh1DJtHjmEJ1kis6GxGL0UhITPwXG/DEJrDQupco5+4AoQJSmYeeYtHZ44CCGRCfRUNf+Hwib+tD+9/ssAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(\"../data\")\n",
    "image_path = data_path / \"b200c-lego-classification-dataset\"\n",
    "\n",
    "image_path_list = list(image_path.glob(\"*/*.jpg\"))\n",
    "\n",
    "random_image_path = random.choice(image_path_list)\n",
    "\n",
    "image_class = random_image_path.parent.stem\n",
    "img = Image.open(random_image_path)\n",
    "\n",
    "print(f\"Random image path: {random_image_path}\")\n",
    "print(f\"Image class: {image_class}\")\n",
    "print(f\"Image height: {img.height}\") \n",
    "print(f\"Image width: {img.width}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45e081ee-4cc2-4dcd-a2f4-8d62aa7ea51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write transform for image\n",
    "data_transform = transforms.Compose([\n",
    "    # Resize the images to 64x64\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    # Flip the images randomly on the horizontal\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "67deec66-20fb-494a-86b6-ced773f44901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "80000\n",
      "Test data:\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = datasets.ImageFolder(root=image_path, \n",
    "                                  transform=data_transform, \n",
    "                                  target_transform=None)\n",
    "\n",
    "subset_size = 100000\n",
    "subset_indices = torch.randperm(len(raw_dataset))[:subset_size]\n",
    "dataset = Subset(raw_dataset, subset_indices)\n",
    "\n",
    "train_ratio = 0.8\n",
    "validation_ratio = 0.2\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "validation_size = dataset_size - train_size\n",
    "\n",
    "train_data, test_data = random_split(dataset, [train_size, validation_size])\n",
    "#class_names = train_data.dataset.classes\n",
    "\n",
    "print(f\"Train data:\\n{len(train_data)}\\nTest data:\\n{len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "25357262-efcb-4816-beed-96c7c8c1c865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x296954be8c0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x296954be4a0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data, \n",
    "                              batch_size=BATCH_SIZE, # how many samples per batch?\n",
    "                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n",
    "                              shuffle=True) # shuffle the data?\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, \n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             num_workers=1, \n",
    "                             shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ebfb5505-1136-42e9-a12e-1a5cee404775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([32, 3, 64, 64]) -> [batch_size, color_channels, height, width]\n",
      "Label shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "img, label = next(iter(train_dataloader))\n",
    "\n",
    "# Batch size will now be 1, try changing the batch_size parameter above and see what happens\n",
    "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f695ec28-04eb-4fe7-987a-dbf88fbe554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer a model\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights\n",
    "efficientnet_b0 = torchvision.models.efficientnet_b0(weights=weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eba7031f-57ef-486e-b37a-4bb1052ae40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.to(device)\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Send data to GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true=y,\n",
    "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate loss and accuracy per epoch and print out what's happening\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.to(device)\n",
    "    model.eval() # put model in eval mode\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode(): \n",
    "        for X, y in data_loader:\n",
    "            # Send data to GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "            \n",
    "            # 2. Calculate loss and accuracy\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += accuracy_fn(y_true=y,\n",
    "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
    "            )\n",
    "        \n",
    "        # Adjust metrics and print out\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "93a23c92-6485-4f7b-b75d-efb4d4fab97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=efficientnet_b0.parameters(), \n",
    "                             lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ecb6192c-a742-464b-983b-59d4e9c96d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: nan | Train accuracy: 0.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 1/3 [00:17<00:35, 17.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: nan | Test accuracy: 0.55%\n",
      "\n",
      "Epoch: 1\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 1/3 [00:30<01:00, 30.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mefficientnet_b0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccuracy_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccuracy_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     test_step(data_loader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[0;32m     13\u001b[0m         model\u001b[38;5;241m=\u001b[39mefficientnet_b0,\n\u001b[0;32m     14\u001b[0m         loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m     15\u001b[0m         accuracy_fn\u001b[38;5;241m=\u001b[39maccuracy_fn,\n\u001b[0;32m     16\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m     17\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[109], line 14\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, data_loader, loss_fn, optimizer, accuracy_fn, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 1. Forward pass\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 2. Calculate loss\u001b[39;00m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torchvision\\models\\efficientnet.py:343\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torchvision\\models\\efficientnet.py:333\u001b[0m, in \u001b[0;36mEfficientNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 333\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m    336\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torchvision\\models\\efficientnet.py:164\u001b[0m, in \u001b[0;36mMBConv.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 164\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[0;32m    166\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstochastic_depth(result)\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.venvs\\machine_learning\\lib\\site-packages\\torch\\nn\\functional.py:2812\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2810\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and test model \n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_dataloader, \n",
    "        model=efficientnet_b0, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device\n",
    "    )\n",
    "    test_step(data_loader=test_dataloader,\n",
    "        model=efficientnet_b0,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff3b47-c481-4edb-97b8-863b04f78bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "ml_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
